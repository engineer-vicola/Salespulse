**Full_transaction Project**

**Overview**

This project implements a fully automated data pipeline that fetches data from Faker module which is used to generate fake data for various purposes. 
The data used for this project is generated on a daily basis, stores it in Amazon S3 and sent to datawarehouse (Redshift). The infrastructure was provisioned using Terraform, while the workflow is managed and scheduled via Apache Airflow.


**Tech Stack**

Python - Scripting

Apache Airflow – Workflow Orchestration

Terraform – Infrastructure as code (IaC)

AWS S3 – Storage layer

Redshift - Datawarehouse

AWS IAM – Access control

AWS Systems Manager (SSM) – Secure credentials storage

**Images from the project;**

![Image](https://github.com/user-attachments/assets/19253cb9-6a8b-4feb-8101-fafe24cd9b92)
![Image](https://github.com/user-attachments/assets/862ac6d2-86f8-46e3-893e-e47fc9cb1286)


