**Full_transation Project**

**Overview**

This project implements a fully automated data pipeline that fetches data from Faker module which is used to generate fake data for various purposes. 
The data used for this project is generated on a daily basis, stores it in Amazon S3 and sent to datawarehouse (Redshift). The infrastructure is provisioned using Terraform, while the workflow is managed and scheduled via Apache Airflow.


**Tech Stack**

Python - Scripting

Apache Airflow – Workflow orchestration

Terraform – Infrastructure as code

AWS S3 – Storage layer

Redshift - Datawarehouse

AWS IAM – Access control

AWS Systems Manager (SSM) – Secure credentials storage

**Images from the project;**

![Image](https://github.com/user-attachments/assets/19253cb9-6a8b-4feb-8101-fafe24cd9b92)


